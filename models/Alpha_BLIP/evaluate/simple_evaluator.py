import sys
import os
import json
import numpy as np
import torch
from PIL import Image, ImageDraw, ImageFont
import re

# åˆ†åˆ«å¯¼å…¥transformersç»„ä»¶ï¼Œé¿å…å¯¼å…¥é—®é¢˜
from transformers import BlipProcessor, BlipForConditionalGeneration
try:
    from transformers import AutoProcessor, AutoModelForVision2Seq
    KOSMOS2_AVAILABLE = True
except ImportError:
    print("âš ï¸  Kosmos2ç›¸å…³åŒ…å¯¼å…¥å¤±è´¥")
    KOSMOS2_AVAILABLE = False

from data.datasets_loader_evaluator import VisualGenomeDataset, RefCOCOPlusDataset
from model_blip.inference import AlphaBlipInference
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer
from pycocoevalcap.cider.cider import Cider
import sacrebleu

class SimpleEvaluator:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = "test_images"
        os.makedirs(self.save_dir, exist_ok=True)
        print(f"å›¾åƒä¿å­˜ç›®å½•: {self.save_dir}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.init_models()
        
        # åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡
        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        self.cider_scorer = Cider()
    
    def init_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # 1. BLIPæ¨¡å‹
        print("  - BLIP...")
        self.blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        self.blip_model = BlipForConditionalGeneration.from_pretrained(
            "Salesforce/blip-image-captioning-base", torch_dtype=torch.float32
        )
        self.blip_model.to(self.device).eval()
        
        # 2. AlphaBlipæ¨¡å‹
        print("  - AlphaBlip...")
        self.alphablip = AlphaBlipInference(
            checkpoint_path="/home/chenzc/cvd/model_blip/outputs/alpha_blip_training/checkpoints/best_model.pt"
        )
        
        # 3. Kosmos2æ¨¡å‹ 
        self.kosmos2_processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.kosmos2_model = AutoModelForVision2Seq.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.kosmos2_model.to(self.device).eval()

        # 4. GLaMM
        self.GlaMM_model = AutoModelForVision2Seq.from_pretrained("MBZUAI/GLaMM-FullScope").to(self.device).eval()
        self.GlaMM_processor = AutoProcessor.from_pretrained("MBZUAI/GLaMM-FullScope")
    
    def save_sample_image(self, image, mask, bbox, sample_idx, dataset_name, text):
        """ä¿å­˜æ ·æœ¬å›¾åƒï¼ŒåŒ…æ‹¬åŸå›¾ã€maskã€bboxå¯è§†åŒ–"""
        try:
            # ä¿å­˜åŸå›¾
            orig_path = os.path.join(self.save_dir, f"{dataset_name}_{sample_idx:03d}_original.jpg")
            image.save(orig_path)
            
            # å¦‚æœæœ‰bboxï¼Œç»˜åˆ¶bbox
            if bbox is not None and len(bbox) == 4:
                img_with_bbox = image.copy()
                draw = ImageDraw.Draw(img_with_bbox)
                
                # è·å–å›¾åƒå°ºå¯¸
                img_width, img_height = image.size
                
                # æ£€æŸ¥bboxæ˜¯å¦å·²å½’ä¸€åŒ–
                x1, y1, x2, y2 = bbox
                if max(bbox) <= 1.0:  # å·²å½’ä¸€åŒ–
                    x1 = int(x1 * img_width)
                    y1 = int(y1 * img_height)
                    x2 = int(x2 * img_width)
                    y2 = int(y2 * img_height)
                
                # ç»˜åˆ¶bbox
                draw.rectangle([x1, y1, x2, y2], outline="red", width=3)
                
                # æ·»åŠ æ–‡æœ¬æ ‡ç­¾
                try:
                    # å°è¯•ä½¿ç”¨é»˜è®¤å­—ä½“
                    font = ImageFont.load_default()
                except:
                    font = None
                
                # ç»˜åˆ¶æ–‡æœ¬èƒŒæ™¯
                text_bbox = draw.textbbox((x1, y1-25), f"Sample {sample_idx}", font=font)
                draw.rectangle(text_bbox, fill="red")
                draw.text((x1, y1-25), f"Sample {sample_idx}", fill="white", font=font)
                
                bbox_path = os.path.join(self.save_dir, f"{dataset_name}_{sample_idx:03d}_with_bbox.jpg")
                img_with_bbox.save(bbox_path)
            
            # å¦‚æœæœ‰maskï¼Œä¿å­˜mask
            if mask is not None:
                mask_path = os.path.join(self.save_dir, f"{dataset_name}_{sample_idx:03d}_mask.jpg")
                if hasattr(mask, 'save'):  # PIL Image
                    mask.save(mask_path)
                elif isinstance(mask, np.ndarray):  # numpy array
                    mask_img = Image.fromarray((mask * 255).astype(np.uint8))
                    mask_img.save(mask_path)
            
            # ä¿å­˜æ–‡æœ¬ä¿¡æ¯
            txt_path = os.path.join(self.save_dir, f"{dataset_name}_{sample_idx:03d}_info.txt")
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(f"Dataset: {dataset_name}\n")
                f.write(f"Sample Index: {sample_idx}\n")
                f.write(f"Image Size: {image.size}\n")
                f.write(f"BBox: {bbox}\n")
                f.write(f"Reference Text: {text}\n")
            
            print(f"    ğŸ’¾ å·²ä¿å­˜æ ·æœ¬{sample_idx}çš„å›¾åƒåˆ° {self.save_dir}/")
            
        except Exception as e:
            print(f"    âš ï¸  ä¿å­˜å›¾åƒå¤±è´¥: {str(e)}")
    
    def blip_inference(self, image):
        """BLIPæ¨ç†"""
        inputs = self.blip_processor(image, return_tensors="pt").to(self.device)
        with torch.no_grad():
            generated_ids = self.blip_model.generate(**inputs, max_length=20, num_beams=3)
        result = self.blip_processor.decode(generated_ids[0], skip_special_tokens=True).strip()
        return result
    
    def alphablip_inference(self, image, mask):
        """AlphaBlipæ¨ç†"""
        result = self.alphablip.generate_caption(image, mask, max_length=20, num_beams=3)
        return result
    
    def kosmos2_inference(self, image, bbox):
        prompt = "<grounding><phrase> It</phrase>is"
        if isinstance(bbox, (list, tuple)) and len(bbox) == 4:
            bbox_tuple = tuple(float(x) for x in bbox)

            bboxes = [bbox_tuple]  # å•ä¸ªbboxçš„åˆ—è¡¨
        else:
            print(f"    âš ï¸  bboxæ ¼å¼é”™è¯¯: {bbox}")
            return ""
        
        inputs = self.kosmos2_processor(
            text=prompt,
            images=image,
            bboxes=bboxes,  # ä½¿ç”¨æ­£ç¡®æ ¼å¼çš„bbox
            return_tensors="pt"
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            generated_ids = self.kosmos2_model.generate(
                pixel_values=inputs["pixel_values"],
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                image_embeds=None,
                image_embeds_position_mask=inputs["image_embeds_position_mask"],
                use_cache=True,
                max_new_tokens=128,
            )
        
        generated_text = self.kosmos2_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        processed_text, entities = self.kosmos2_processor.post_process_generation(generated_text)
        
        return processed_text
    
    def glamm_inference(self, image, bbox):
 
        
        # éªŒè¯bboxè¾“å…¥
        if bbox is None or len(bbox) != 4:
            raise ValueError("bbox is required and must be a list of 4 coordinates [x1, y1, x2, y2]")
        
        # GLaMMçš„æ ‡å‡†åŒºåŸŸæè¿°æç¤º
        prompt = "<image>Can you provide me with a detailed description of the region in the picture marked by <bbox>?"
        
        # è·å–å›¾åƒå°ºå¯¸
        if isinstance(image, Image.Image):
            width, height = image.size
        else:
            height, width = image.shape[:2]
        
        # æŒ‰ç…§GLaMMçš„æ ‡å‡†æ–¹å¼å¤„ç†è¾¹ç•Œæ¡†
        # æ­¥éª¤1: ç¼©æ”¾åˆ°336Ã—336 (CLIPç¼–ç å™¨è¾“å…¥å°ºå¯¸)
        x_scale, y_scale = 336 / width, 336 / height
        bbox_scaled = [
            bbox[0] * x_scale,  # x1
            bbox[1] * y_scale,  # y1
            bbox[2] * x_scale,  # x2
            bbox[3] * y_scale   # y2
        ]
        
        # æ­¥éª¤2: å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´
        height_sc, width_sc = (336, 336)
        norm_bbox = np.array(bbox_scaled) / np.array([width_sc, height_sc, width_sc, height_sc])
        

        inputs = self.GlaMM_processor(
            text=prompt,
            images=image,
            bboxes=[norm_bbox.tolist()],  # ä¼ é€’å½’ä¸€åŒ–åçš„è¾¹ç•Œæ¡†
            return_tensors="pt"
        )

        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        inputs = {k: v.to(self.device) if torch.is_tensor(v) else v for k, v in inputs.items()}
        
        # ç”Ÿæˆæ–‡æœ¬
        with torch.no_grad():
            generated_ids = self.GlaMM_model.generate(
                **inputs,
                max_new_tokens=512,
                num_beams=3,
                temperature=0.7,
                do_sample=False,
            )
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = self.GlaMM_processor.batch_decode(
            generated_ids, 
            skip_special_tokens=True
        )[0]
        
        # æ¸…ç†è¾“å‡ºæ–‡æœ¬
        # ç§»é™¤è¾“å…¥æç¤º
        if prompt in generated_text:
            result = generated_text.replace(prompt, "").strip()
        else:
            # å¦‚æœç›´æ¥æ›¿æ¢å¤±è´¥ï¼Œå°è¯•åˆ†å‰²
            parts = generated_text.split("ASSISTANT:")
            if len(parts) > 1:
                result = parts[-1].strip()
            else:
                result = generated_text.strip()
        
        # è¿›ä¸€æ­¥æ¸…ç†
        # ç§»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Štoken
        result = re.sub(r'<.*?>', '', result)
        result = result.replace('[SEG]', '')
        result = result.replace('\n', ' ').replace('  ', ' ')
        result = ' '.join(result.split()).strip()
        
        return result


    def load_dataset(self, dataset_name, max_samples=100):
        """åŠ è½½æ•°æ®é›†"""
        print(f"åŠ è½½ {dataset_name} æ•°æ®é›†...")
        
        if dataset_name == "VisualGenome":
            dataset = VisualGenomeDataset(
                region_descriptions_file="/home/chenzc/cvd/model_blip/data/data/visual_genome/VG_val_3000/region_descriptions.json",
                image_data_file="/home/chenzc/cvd/model_blip/data/data/visual_genome/VG_val_3000/image_data.json",
                images_dir="/home/chenzc/cvd/model_blip/data/data/visual_genome/VG_100K_all",
                image_size=(384, 384)
            )
            mask_key = 'ellipse_mask'  # AlphaBlipåœ¨VGä¸Šä½¿ç”¨ellipse_mask
        else:  # RefCOCOPlus
            dataset = RefCOCOPlusDataset(
                refs_file="/home/chenzc/cvd/model_blip/data/data/COCO/refcoco+/refs(unc).p",
                instances_file="/home/chenzc/cvd/model_blip/data/data/COCO/refcoco+/instances.json",
                images_dir="/home/chenzc/cvd/model_blip/data/data/COCO/train2014",
                image_size=(384, 384)
            )
            mask_key = 'gray_mask'  # AlphaBlipåœ¨RefCOCO+ä¸Šä½¿ç”¨gray_mask
        
        # é‡‡æ ·æ•°æ®
        total_samples = len(dataset)
        if max_samples and max_samples < total_samples:
            import random
            random.seed(42)
            indices = random.sample(range(total_samples), max_samples)
        else:
            indices = range(total_samples)
        
        # æå–æ ·æœ¬
        samples = []
        for i in indices:
            try:
                sample = dataset[i]
                samples.append({
                    'image': sample['orig_image'],
                    'mask': sample.get(mask_key, None),
                    'bbox': sample.get('normalized_bbox', None),
                    'text': sample['text'][0] if isinstance(sample['text'], list) else sample['text']
                })
            except:
                continue
        
        print(f"  æˆåŠŸåŠ è½½ {len(samples)} ä¸ªæ ·æœ¬")
        return samples
    
    def run_inference(self, samples, dataset_name):
        """è¿è¡Œæ¨ç†"""
        print(f"å¼€å§‹æ¨ç† {dataset_name}...")
        
        results = {
            'blip': [],
            'alphablip': [],
            'kosmos2': [],
            'references': []
        }
        
        for i, sample in enumerate(samples):
            if (i + 1) % 20 == 0:  # æ›´é¢‘ç¹çš„è¿›åº¦æ˜¾ç¤º
                print(f"  è¿›åº¦: {i+1}/{len(samples)}")
            
            # ä¿å­˜å‰å‡ ä¸ªæ ·æœ¬çš„å›¾åƒï¼ˆå¯ä»¥è°ƒæ•´ä¿å­˜æ•°é‡ï¼‰
            if i < 10:  # ä¿å­˜å‰10ä¸ªæ ·æœ¬
                self.save_sample_image(
                    sample['image'], 
                    sample['mask'], 
                    sample['bbox'], 
                    i+1, 
                    dataset_name, 
                    sample['text']
                )
            
            # BLIPæ¨ç† (åªç”¨åŸå›¾)
            blip_pred = self.blip_inference(sample['image'])
            results['blip'].append(blip_pred)
            
            # AlphaBlipæ¨ç† (ç”¨åŸå›¾+mask)
            alphablip_pred = self.alphablip_inference(sample['image'], sample['mask'])
            results['alphablip'].append(alphablip_pred)
            
            # Kosmos2æ¨ç† (ç”¨åŸå›¾+bbox)
            kosmos2_pred = self.kosmos2_inference(sample['image'], sample['bbox'])
            results['kosmos2'].append(kosmos2_pred)
            
            # å‚è€ƒæ–‡æœ¬
            results['references'].append(sample['text'])
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„ç»“æœ
            if i < 3:
                print(f"    æ ·æœ¬{i+1}:")
                print(f"      å‚è€ƒ: {sample['text']}")
                print(f"      BLIP: {blip_pred}")
                print(f"      AlphaBlip: {alphablip_pred}")
                print(f"      Kosmos2: {kosmos2_pred}")
                print(f"      Kosmos2(æ¸…ç†å): {self.clean_prediction(kosmos2_pred)}")
                print()
        
        return results
    
    def clean_prediction(self, text):
        """æ¸…ç†é¢„æµ‹æ–‡æœ¬ï¼Œå»æ‰å›ºå®šå‰ç¼€"""
        if not text:
            return ""
        
        # è½¬æ¢ä¸ºå°å†™è¿›è¡ŒåŒ¹é…
        text_lower = text.lower().strip()
        
        # å®šä¹‰è¦å»æ‰çš„å‰ç¼€æ¨¡å¼
        prefixes_to_remove = [
            "it is",
            "it's",
            "this is", 
            "there is",
            "there are",
            "the image shows",
            "the image depicts"
        ]
        
        # å»æ‰åŒ¹é…çš„å‰ç¼€
        for prefix in prefixes_to_remove:
            if text_lower.startswith(prefix):
                # å»æ‰å‰ç¼€ï¼Œä¿ç•™åŸå§‹å¤§å°å†™
                remaining = text[len(prefix):].strip()
                # å¦‚æœå»æ‰å‰ç¼€åè¿˜æœ‰å†…å®¹ï¼Œè¿”å›å‰©ä½™éƒ¨åˆ†
                if remaining:
                    return remaining
                break
        
        return text.strip()
    
    def calculate_metrics(self, predictions, references):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        if not predictions:
            return {}
        
        # æ¸…ç†é¢„æµ‹æ–‡æœ¬ï¼Œå»æ‰å›ºå®šå‰ç¼€
        cleaned_predictions = [self.clean_prediction(pred) for pred in predictions]
        
        # è¿‡æ»¤æ‰ç©ºçš„é¢„æµ‹
        valid_pairs = [(pred, ref) for pred, ref in zip(cleaned_predictions, references) if pred.strip()]
        
        if not valid_pairs:
            print("    âš ï¸  æ‰€æœ‰é¢„æµ‹éƒ½ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡")
            return {}
        
        cleaned_preds, valid_refs = zip(*valid_pairs)
        
        # BLEU
        bleu = sacrebleu.corpus_bleu(cleaned_preds, [valid_refs])
        
        # METEOR
        meteor_scores = []
        for pred, ref in zip(cleaned_preds, valid_refs):
            pred_tokens = pred.lower().split()
            ref_tokens = ref.lower().split()
            if pred_tokens:  # ç¡®ä¿é¢„æµ‹ä¸ä¸ºç©º
                score = meteor_score([ref_tokens], pred_tokens)
                meteor_scores.append(score)
        
        # ROUGE-L
        rouge_scores = []
        for pred, ref in zip(cleaned_preds, valid_refs):
            if pred.strip():  # ç¡®ä¿é¢„æµ‹ä¸ä¸ºç©º
                scores = self.rouge_scorer.score(ref.lower(), pred.lower())
                rouge_scores.append(scores['rougeL'].fmeasure)
        
        # CIDEr
        gts = {i: [ref.lower()] for i, ref in enumerate(valid_refs)}
        res = {i: [pred.lower()] for i, pred in enumerate(cleaned_preds)}
        cider_score, _ = self.cider_scorer.compute_score(gts, res)
        
        return {
            'BLEU-1': bleu.precisions[0],
            'BLEU-2': bleu.precisions[1],
            'METEOR': np.mean(meteor_scores) if meteor_scores else 0.0,
            'ROUGE-L': np.mean(rouge_scores) if rouge_scores else 0.0,
            'CIDEr': cider_score
        }
    
    def evaluate_dataset(self, dataset_name, max_samples=1000):
        """è¯„ä¼°å•ä¸ªæ•°æ®é›†"""
        # åŠ è½½æ•°æ®
        samples = self.load_dataset(dataset_name, max_samples)
        
        # æ¨ç†
        results = self.run_inference(samples, dataset_name)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {}
        for model_name in ['blip', 'alphablip', 'kosmos2']:
            metrics[model_name] = self.calculate_metrics(
                results[model_name], results['references']
            )
        return metrics
    
    def run_evaluation(self, max_samples=100):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        all_results = {}
        
        # è¯„ä¼°VisualGenome
        print("\n" + "="*50)
        all_results['VisualGenome'] = self.evaluate_dataset('VisualGenome', max_samples)
        
        # è¯„ä¼°RefCOCOPlus
        print("\n" + "="*50)
        all_results['RefCOCOPlus'] = self.evaluate_dataset('RefCOCOPlus', max_samples)
        
        # æ‰“å°ç»“æœ
        self.print_results(all_results)
        
        return all_results
    
    def print_results(self, all_results):
        """æ‰“å°ç»“æœè¡¨æ ¼"""
        print("\n" + "="*100)
        print("ğŸ“Š è¯„ä¼°ç»“æœ")
        print("="*100)
        
        # è¡¨å¤´
        print(f"{'æ¨¡å‹':<12}", end="")
        for dataset in ['visual_genome', 'refcoco']:
            print(f"{dataset:^50}", end="")
        print()
        
        print(f"{'':12}", end="")
        for _ in range(2):
            print(f"{'bleu-1':<10}{'bleu-2':<10}{'meteor':<10}{'rouge-l':<10}{'cider':<10}", end="")
        print()
        print("-" * 112)
        
        # æ•°æ®è¡Œ
        models = ['blip', 'alphablip','kosmos2']
        
        for model in models:
            print(f"{model:<12}", end="")
            
            for dataset in ['VisualGenome', 'RefCOCOPlus']:
                if dataset in all_results and model in all_results[dataset]:
                    m = all_results[dataset][model]
                    print(f"{m['BLEU-1']:<10.3f}{m['BLEU-2']:<10.3f}{m['METEOR']:<10.3f}"
                          f"{m['ROUGE-L']:<10.3f}{m['CIDEr']:<10.3f}", end="")
                else:
                    print(f"{'':50}", end="")
            print()
        
        print("="*100)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç®€åŒ–ç‰ˆæ¨¡å‹è¯„ä¼°")
    print("="*50)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = SimpleEvaluator()
    
    # è¿è¡Œè¯„ä¼° (æ¯ä¸ªæ•°æ®é›†1000ä¸ªæ ·æœ¬)
    results = evaluator.run_evaluation(max_samples=1000)
    
    print("\nâœ… è¯„ä¼°å®Œæˆ!")
    print(f"ğŸ“ æµ‹è¯•å›¾åƒå·²ä¿å­˜åˆ°: {evaluator.save_dir}/")

if __name__ == "__main__":
    main()