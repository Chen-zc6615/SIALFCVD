import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BlipForConditionalGeneration, BlipConfig, BlipVisionConfig
from transformers.models.blip.modeling_blip import (
    BlipPreTrainedModel,
    BlipForConditionalGenerationModelOutput,
    BlipVisionEmbeddings,
    BlipEncoder,
    BlipTextLMHeadModel
)
from transformers.generation import GenerationMixin
from transformers.modeling_outputs import BaseModelOutputWithPooling
from typing import Optional, Union, Tuple



class AlphaBlipVisionEmbeddings(BlipVisionEmbeddings):
    """
    æ”¯æŒAlphaé€šé“çš„BLIPè§†è§‰åµŒå…¥å±‚
    
    åŸºäºå®˜æ–¹BlipVisionEmbeddingsï¼Œé‡‡ç”¨æœ€å°ä¾µå…¥å¼è®¾è®¡ï¼š
    - ç»§æ‰¿æ‰€æœ‰åŸå§‹åŠŸèƒ½
    - åªæ·»åŠ Alphaé€šé“æ”¯æŒ
    - é›¶åˆå§‹åŒ–ç¡®ä¿å‘åå…¼å®¹æ€§
    """
    
    def __init__(self, config: BlipVisionConfig):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œè·å¾—æ‰€æœ‰åŸå§‹åŠŸèƒ½
        super().__init__(config)
        
        # æ·»åŠ Alphaé€šé“çš„patch embedding
        # ä½¿ç”¨ä¸RGBç›¸åŒçš„å‚æ•°ï¼Œç¡®ä¿å…¼å®¹æ€§
        self.patch_embedding_alpha = nn.Conv2d(
            in_channels=1,  # Alphaé€šé“
            out_channels=self.embed_dim,  # ä¸RGBç›¸åŒçš„è¾“å‡ºç»´åº¦
            kernel_size=self.patch_size,  # ä¸RGBç›¸åŒçš„kernel size
            stride=self.patch_size,       # ä¸RGBç›¸åŒçš„stride
            bias=False  # ä¸åŸå§‹patch_embeddingä¿æŒä¸€è‡´ï¼ˆHuggingFace BLIPé»˜è®¤æ— biasï¼‰
        )
        
        with torch.no_grad():
            nn.init.zeros_(self.patch_embedding_alpha.weight)
    
    def forward(self, 
                pixel_values: torch.FloatTensor, 
                alpha_values: Optional[torch.FloatTensor] = None,
                interpolate_pos_encoding: bool = False) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼Œæ”¯æŒAlphaé€šé“
        
        Args:
            pixel_values: [B, 3, H, W] RGBå›¾åƒ
            alpha_values: [B, 1, H, W] Alphaé€šé“ï¼ˆå¯é€‰ï¼‰
            interpolate_pos_encoding: æ˜¯å¦æ’å€¼ä½ç½®ç¼–ç 
            
        Returns:
            embeddings: [B, N+1, D] å›¾åƒåµŒå…¥ï¼ˆåŒ…å«CLS tokenï¼‰
        """
        batch_size, _, height, width = pixel_values.shape
        target_dtype = self.patch_embedding.weight.dtype
        
        # === å…³é”®ä¿®æ”¹ï¼šæ”¯æŒAlphaé€šé“çš„patch embedding ===
        # RGB patch embeddingï¼ˆä¿æŒåŸå§‹é€»è¾‘ï¼‰
        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))

        # Alpha patch embeddingï¼ˆå¦‚æœæä¾›Alphaé€šé“ï¼‰
        if alpha_values is not None:
            # ç¡®ä¿Alphaé€šé“ç»´åº¦æ­£ç¡®
            if alpha_values.shape[1] != 1:
                raise ValueError(f"Alpha channel should have 1 channel, got {alpha_values.shape[1]}")
            
            alpha_embeds = self.patch_embedding_alpha(alpha_values.to(dtype=target_dtype))
            # ç›´æ¥ç›¸åŠ èåˆ - éµå¾ªåŸå§‹CLIPè®¾è®¡å“²å­¦
            patch_embeds = patch_embeds + alpha_embeds
        
        # === åç»­é€»è¾‘å®Œå…¨ä¿æŒåŸå§‹BLIPå®ç° ===
        # ç»´åº¦å˜æ¢ï¼š[B, C, H', W'] -> [B, N, C]
        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)
        
        # æ·»åŠ CLS token
        class_embeds = self.class_embedding.expand(batch_size, 1, -1).to(target_dtype)
        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)
        
        # ä½ç½®ç¼–ç å¤„ç†
        if interpolate_pos_encoding:
            position_embedding = self.interpolate_pos_encoding(embeddings, height, width)
        else:
            position_embedding = self.position_embedding
            
        # æ·»åŠ ä½ç½®ç¼–ç 
        embeddings = embeddings + position_embedding[:, : embeddings.size(1), :].to(target_dtype)
        
        return embeddings


class AlphaBlipVisionModel(BlipPreTrainedModel):
    """
    æ”¯æŒAlphaé€šé“çš„BLIPè§†è§‰æ¨¡å‹
    å®Œå…¨åŸºäºçœŸå®çš„BlipVisionModelæ¶æ„
    """
    main_input_name = "pixel_values"
    config_class = BlipVisionConfig

    def __init__(self, config: BlipVisionConfig):
        super().__init__(config)
        self.config = config
        embed_dim = config.hidden_size

        # ä½¿ç”¨Alphaç‰ˆæœ¬çš„embeddings
        self.embeddings = AlphaBlipVisionEmbeddings(config)
        
        # å…¶ä»–ç»„ä»¶ä¿æŒä¸åŸå§‹BlipVisionModelå®Œå…¨ä¸€è‡´
        self.encoder = BlipEncoder(config)
        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)

        self.post_init()

    def forward(
        self,
        pixel_values: Optional[torch.FloatTensor] = None,
        alpha_values: Optional[torch.FloatTensor] = None,  
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        interpolate_pos_encoding: bool = False,
    ) -> Union[Tuple, BaseModelOutputWithPooling]:
        """
        å‰å‘ä¼ æ’­ï¼Œæ”¯æŒAlphaé€šé“
        å®Œå…¨åŸºäºåŸå§‹BlipVisionModel.forwardçš„é€»è¾‘
        
        Args:
            pixel_values: [B, 3, H, W] RGBå›¾åƒ
            alpha_values: [B, 1, H, W] Alphaé€šé“ (æ–°å¢å‚æ•°)
            å…¶ä»–å‚æ•°: ä¸åŸå§‹BlipVisionModelä¿æŒä¸€è‡´
        """
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if pixel_values is None:
            raise ValueError("You have to specify pixel_values")

        # ä½¿ç”¨Alphaç‰ˆæœ¬çš„embeddings (ä¼ é€’Alphaé€šé“)
        hidden_states = self.embeddings(
            pixel_values, 
            alpha_values=alpha_values,  # ä¼ é€’Alphaé€šé“
            interpolate_pos_encoding=interpolate_pos_encoding
        )

        # encoderé€»è¾‘å®Œå…¨ä¸å˜ï¼Œä¸åŸå§‹BlipVisionModelä¿æŒä¸€è‡´
        encoder_outputs = self.encoder(
            inputs_embeds=hidden_states,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        last_hidden_state = encoder_outputs[0]
        last_hidden_state = self.post_layernorm(last_hidden_state)

        pooled_output = last_hidden_state[:, 0, :]
        pooled_output = self.post_layernorm(pooled_output)

        if not return_dict:
            return (last_hidden_state, pooled_output) + encoder_outputs[1:]

        return BaseModelOutputWithPooling(
            last_hidden_state=last_hidden_state,
            pooler_output=pooled_output,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
        )

    def get_input_embeddings(self):
        return self.embeddings


class AlphaBlipForConditionalGeneration(BlipPreTrainedModel, GenerationMixin):
    """
    æ”¯æŒAlphaé€šé“çš„BLIPæ¡ä»¶ç”Ÿæˆæ¨¡å‹
    å®Œå…¨åŸºäºåŸå§‹BlipForConditionalGenerationçš„æ¶æ„å’Œå®ç°
    """
    config_class = BlipConfig
    _tied_weights_keys = ["text_decoder.cls.predictions.decoder.bias"]
    main_input_name = "pixel_values"

    def __init__(self, config: BlipConfig):
        super().__init__(config)

       
        self.vision_model = AlphaBlipVisionModel(config.vision_config)

        # æ–‡æœ¬è§£ç å™¨ä¿æŒä¸åŸå§‹å®Œå…¨ä¸€è‡´
        self.text_decoder = BlipTextLMHeadModel(config.text_config)

        self.decoder_input_ids = config.text_config.bos_token_id
        self.decoder_pad_token_id = config.text_config.pad_token_id

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.text_decoder.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.text_decoder.set_input_embeddings(value)


    def forward(
        self,
        pixel_values: torch.FloatTensor,
        alpha_values: Optional[torch.FloatTensor] = None,  # æ–°å¢Alphaé€šé“è¾“å…¥
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.LongTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        labels: Optional[torch.LongTensor] = None,
        return_dict: Optional[bool] = None,
        interpolate_pos_encoding: bool = False,
    ) -> Union[Tuple, BlipForConditionalGenerationModelOutput]:
        """
        å‰å‘ä¼ æ’­ï¼Œæ”¯æŒAlphaé€šé“
        å®Œå…¨åŸºäºåŸå§‹BlipForConditionalGeneration.forwardçš„é€»è¾‘
        
        Examples:
        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor
        >>> import torch

        >>> processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        >>> model = AlphaBlipForConditionalGeneration.from_pretrained_alpha("Salesforce/blip-image-captioning-base")

        >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)
        >>> alpha = torch.ones(1, 1, 384, 384)  # Alpha channel
        >>> text = "A picture of"

        >>> inputs = processor(images=image, text=text, return_tensors="pt")
        >>> outputs = model(pixel_values=inputs.pixel_values, alpha_values=alpha, input_ids=inputs.input_ids)
        ```
        """

        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )

        # ä½¿ç”¨Alphaç‰ˆæœ¬çš„è§†è§‰æ¨¡å‹ (ä¼ é€’Alphaé€šé“)
        vision_outputs = self.vision_model(
            pixel_values=pixel_values,
            alpha_values=alpha_values,  # ä¼ é€’Alphaé€šé“
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            interpolate_pos_encoding=interpolate_pos_encoding,
        )

        image_embeds = vision_outputs[0]

        # æ–‡æœ¬è§£ç å™¨é€»è¾‘å®Œå…¨ä¿æŒä¸åŸå§‹ä¸€è‡´
        outputs = self.text_decoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            encoder_hidden_states=image_embeds,
            labels=labels,
            return_dict=return_dict,
            reduction="mean",
        )

        if not return_dict:
            outputs = (outputs[0], outputs[1]) if labels is not None else (outputs[0],)
            outputs += (image_embeds, vision_outputs[0]) + vision_outputs[2:]
            return tuple(output for output in outputs if output is not None)

        return BlipForConditionalGenerationModelOutput(
            loss=outputs.loss,
            logits=outputs.logits,
            image_embeds=image_embeds,
            last_hidden_state=vision_outputs.last_hidden_state,
            hidden_states=vision_outputs.hidden_states,
            attentions=vision_outputs.attentions,
        )

    @torch.no_grad()
    def generate(
        self,
        pixel_values: torch.FloatTensor,
        alpha_values: Optional[torch.FloatTensor] = None,  # æ–°å¢Alphaé€šé“è¾“å…¥
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.LongTensor] = None,
        interpolate_pos_encoding: bool = False,
        **generate_kwargs,
    ) -> torch.LongTensor:
        """
        ç”Ÿæˆå›¾åƒæè¿°ï¼Œæ”¯æŒAlphaé€šé“
        å®Œå…¨åŸºäºåŸå§‹BlipForConditionalGeneration.generateçš„é€»è¾‘
        
        Parameters:
            pixel_values: [B, 3, H, W] RGBå›¾åƒè¾“å…¥
            alpha_values: [B, 1, H, W] Alphaé€šé“è¾“å…¥ (æ–°å¢å‚æ•°)
            input_ids: æ–‡æœ¬æç¤ºçš„tokenåºåˆ—
            attention_mask: æ³¨æ„åŠ›æ©ç 
            interpolate_pos_encoding: æ˜¯å¦æ’å€¼ä½ç½®ç¼–ç 
            **generate_kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°

        Examples:
        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor
        >>> import torch

        >>> processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        >>> model = AlphaBlipForConditionalGeneration.from_pretrained_alpha("Salesforce/blip-image-captioning-base")

        >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)
        >>> alpha = torch.ones(1, 1, 384, 384)  # Alpha channel

        >>> inputs = processor(images=image, return_tensors="pt")
        >>> outputs = model.generate(pixel_values=inputs.pixel_values, alpha_values=alpha)
        >>> print(processor.decode(outputs[0], skip_special_tokens=True))
        two cats sleeping on a couch
        ```
        """

        batch_size = pixel_values.shape[0]
        
        # ä½¿ç”¨Alphaç‰ˆæœ¬çš„è§†è§‰æ¨¡å‹ (ä¼ é€’Alphaé€šé“)
        vision_outputs = self.vision_model(
            pixel_values=pixel_values,
            alpha_values=alpha_values,  # ä¼ é€’Alphaé€šé“
            interpolate_pos_encoding=interpolate_pos_encoding,
        )

        image_embeds = vision_outputs[0]

        # åç»­é€»è¾‘å®Œå…¨ä¿æŒä¸åŸå§‹ä¸€è‡´
        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)

        if isinstance(input_ids, list):
            input_ids = torch.LongTensor(input_ids)
        elif input_ids is None:
            input_ids = (
                torch.LongTensor([[self.decoder_input_ids, self.config.text_config.eos_token_id]])
                .repeat(batch_size, 1)
                .to(image_embeds.device)
            )

        input_ids[:, 0] = self.config.text_config.bos_token_id
        attention_mask = attention_mask[:, :-1] if attention_mask is not None else None

        outputs = self.text_decoder.generate(
            input_ids=input_ids[:, :-1],
            eos_token_id=self.config.text_config.sep_token_id,
            pad_token_id=self.config.text_config.pad_token_id,
            attention_mask=attention_mask,
            encoder_hidden_states=image_embeds,
            encoder_attention_mask=image_attention_mask,
            **generate_kwargs,
        )

        return outputs

    @classmethod
    def from_pretrained_alpha(
        cls,
        pretrained_model_name: str,
        **kwargs
    ):
        """
        ä»é¢„è®­ç»ƒBLIPæ¨¡å‹åˆ›å»ºAlphaç‰ˆæœ¬çš„ç±»æ–¹æ³•
        
        Args:
            pretrained_model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™from_pretrained
            
        Returns:
            Alpha BLIPæ¨¡å‹
            
        Examples:
        ```python
        >>> model = AlphaBlipForConditionalGeneration.from_pretrained_alpha(
        ...     "Salesforce/blip-image-captioning-base"
        ... )
        ```
        """
        # åŠ è½½åŸå§‹é¢„è®­ç»ƒæ¨¡å‹
        print(f"ğŸš€ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒBLIPæ¨¡å‹: {pretrained_model_name}")
        
        # âŒ ä¿®å¤3: æ·»åŠ å¼‚å¸¸å¤„ç†
        try:
            original_model = BlipForConditionalGeneration.from_pretrained(pretrained_model_name, **kwargs)
        except Exception as e:
            print(f"âŒ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
            raise
            
        config = original_model.config
        
        # åˆ›å»ºAlphaç‰ˆæœ¬
        print("ğŸ”§ æ­£åœ¨åˆ›å»ºAlpha BLIPæ¨¡å‹...")
        alpha_model = cls(config)
        
        # å¤åˆ¶é¢„è®­ç»ƒæƒé‡
        print("ğŸ“‹ æ­£åœ¨å¤åˆ¶é¢„è®­ç»ƒæƒé‡...")
        
        try:
            # å¤åˆ¶è§†è§‰æ¨¡å‹æƒé‡ (é™¤äº†æ–°å¢çš„alphaå±‚)
            vision_state_dict = original_model.vision_model.state_dict()
            missing_keys, unexpected_keys = alpha_model.vision_model.load_state_dict(vision_state_dict, strict=False)
            
   
            if missing_keys:
                print(f"âš ï¸  è§†è§‰æ¨¡å‹ç¼ºå¤±çš„é”® (é¢„æœŸåŒ…å«Alphaå±‚): {missing_keys}")
            if unexpected_keys:
                print(f"âš ï¸  è§†è§‰æ¨¡å‹æ„å¤–çš„é”®: {unexpected_keys}")
            
            # å¤åˆ¶æ–‡æœ¬è§£ç å™¨æƒé‡
            text_state_dict = original_model.text_decoder.state_dict()
            alpha_model.text_decoder.load_state_dict(text_state_dict, strict=True)
            
            # å¤åˆ¶å…¶ä»–å‚æ•°
            alpha_model.decoder_input_ids = original_model.decoder_input_ids
            alpha_model.decoder_pad_token_id = original_model.decoder_pad_token_id
            
        except Exception as e:
            print(f"âŒ æƒé‡å¤åˆ¶å¤±è´¥: {e}")
            raise
        
        print("âœ… Alpha BLIPæ¨¡å‹åˆ›å»ºå®Œæˆ!")
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in alpha_model.parameters())
        alpha_params = sum(p.numel() for p in alpha_model.vision_model.embeddings.patch_embedding_alpha.parameters())
        
        print(f"ğŸ“Š å‚æ•°ç»Ÿè®¡:")
        print(f"   æ€»å‚æ•°é‡: {total_params:,}")
        print(f"   Alphaå‚æ•°é‡: {alpha_params:,}")
        print(f"   Alphaå‚æ•°å æ¯”: {alpha_params/total_params*100:.3f}%")
        
 
        alpha_weight_norm = torch.norm(alpha_model.vision_model.embeddings.patch_embedding_alpha.weight).item()
        print(f"ğŸ” Alphaæƒé‡åˆå§‹åŒ–æ£€æŸ¥: ||weight|| = {alpha_weight_norm:.6f} (åº”è¯¥æ¥è¿‘0)")
        
        return alpha_model


def get_alpha_parameters(model: AlphaBlipForConditionalGeneration):
    """è·å–Alphaç›¸å…³å‚æ•°"""
    alpha_params = []
    for name, param in model.named_parameters():
        if 'alpha' in name.lower():
            alpha_params.append((name, param))
    return alpha_params


def test_alpha_blip_model():
    """æµ‹è¯•Alpha BLIPæ¨¡å‹çš„å„é¡¹åŠŸèƒ½"""
    print("ğŸ¯ åŸºäºå®Œæ•´æºç çš„BLIP Alphaé€šé“æ‰©å±•æµ‹è¯•")
    
    try:
        # åˆ›å»ºAlpha BLIPæ¨¡å‹
        print("\nğŸ“¦ æ¨¡å‹åŠ è½½æµ‹è¯•:")
        model = AlphaBlipForConditionalGeneration.from_pretrained_alpha(
            pretrained_model_name="Salesforce/blip-image-captioning-base"
        )
        
        # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = model.to(device)
        print(f"ğŸ”§ æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
        
        # æµ‹è¯•è¾“å…¥
        batch_size = 2
        # âŒ ä¿®å¤7: ä½¿ç”¨BLIPæ ‡å‡†çš„å›¾åƒå°ºå¯¸
        image_size = 224  # BLIPæ ‡å‡†å°ºå¯¸æ˜¯224x224ï¼Œä¸æ˜¯384x384
        rgb_images = torch.randn(batch_size, 3, image_size, image_size).to(device)
        alpha_channels = torch.randn(batch_size, 1, image_size, image_size).to(device)
        
        # âŒ ä¿®å¤ï¼šåˆ›å»ºæ­£ç¡®çš„æ–‡æœ¬è¾“å…¥ï¼ˆå¯é€‰ï¼Œç”¨äºæµ‹è¯•ï¼‰
        # BLIPå¯ä»¥åœ¨æ²¡æœ‰input_idsçš„æƒ…å†µä¸‹å·¥ä½œï¼Œä½†ä¸ºäº†æµ‹è¯•ç¨³å®šæ€§ï¼Œæˆ‘ä»¬æä¾›è¾“å…¥
        input_ids = torch.tensor([[30522, 102]] * batch_size).to(device)  # [BOS] [EOS]
        attention_mask = torch.tensor([[1, 1]] * batch_size).to(device)
        
        print(f"\nğŸ§ª åŠŸèƒ½æµ‹è¯•:")
        print(f"   æ‰¹é‡å¤§å°: {batch_size}")
        print(f"   å›¾åƒå°ºå¯¸: {image_size}x{image_size}")
        print(f"   æ–‡æœ¬è¾“å…¥: {input_ids.shape}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        with torch.no_grad():
            print("1ï¸âƒ£ æµ‹è¯•å‰å‘ä¼ æ’­ï¼ˆå¸¦æ–‡æœ¬è¾“å…¥ï¼‰...")
            outputs = model(
                pixel_values=rgb_images,
                alpha_values=alpha_channels,
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_dict=True
            )
            print(f"   âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"      å›¾åƒç‰¹å¾: {outputs.image_embeds.shape}")
            print(f"      æœ€åéšè—çŠ¶æ€: {outputs.last_hidden_state.shape}")
            print(f"      logits: {outputs.logits.shape}")
            
            print("1ï¸âƒ£-b æµ‹è¯•å‰å‘ä¼ æ’­ï¼ˆä¸å¸¦æ–‡æœ¬è¾“å…¥ï¼Œæ¨¡æ‹ŸåŸå§‹BLIPï¼‰...")
            try:
                outputs_no_text = model(
                    pixel_values=rgb_images,
                    alpha_values=alpha_channels,
                    input_ids=None,  # æµ‹è¯•åŸå§‹BLIPçš„è¡Œä¸º
                    return_dict=True
                )
                print(f"   âœ… æ— æ–‡æœ¬è¾“å…¥çš„å‰å‘ä¼ æ’­æˆåŠŸ")
            except Exception as e:
                print(f"   âš ï¸  æ— æ–‡æœ¬è¾“å…¥çš„å‰å‘ä¼ æ’­å¤±è´¥: {e}")
                print(f"   ğŸ“ è¿™å¯èƒ½æ˜¯transformersç‰ˆæœ¬å·®å¼‚å¯¼è‡´çš„ï¼Œä½¿ç”¨æ–‡æœ¬è¾“å…¥è¿›è¡Œåç»­æµ‹è¯•")
            
            print("2ï¸âƒ£ æµ‹è¯•ç”Ÿæˆ...")
            generated_ids = model.generate(
                pixel_values=rgb_images,
                alpha_values=alpha_channels,
                max_length=20,
                num_beams=2,
                do_sample=False
            )
            print(f"   âœ… ç”Ÿæˆæµ‹è¯•æˆåŠŸ: {generated_ids.shape}")
            
            print("3ï¸âƒ£ æµ‹è¯•æ— Alphaé€šé“çš„å‘åå…¼å®¹æ€§...")
            outputs_no_alpha = model(
                pixel_values=rgb_images,
                alpha_values=None,  # æµ‹è¯•å‘åå…¼å®¹æ€§
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_dict=True
            )
            print(f"   âœ… æ— Alphaé€šé“æµ‹è¯•æˆåŠŸ")
            
            # âŒ ä¿®å¤8: éªŒè¯Alphaé€šé“å¯¹è¾“å‡ºçš„å½±å“
            print("4ï¸âƒ£ æµ‹è¯•Alphaé€šé“çš„å½±å“...")
            # ç”¨é›¶Alphaé€šé“
            zero_alpha = torch.zeros_like(alpha_channels)
            outputs_zero_alpha = model(
                pixel_values=rgb_images,
                alpha_values=zero_alpha,
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_dict=True
            )
            
            # æ¯”è¾ƒè¾“å‡ºå·®å¼‚
            diff_norm = torch.norm(outputs_no_alpha.image_embeds - outputs_zero_alpha.image_embeds).item()
            print(f"   æ— Alpha vs é›¶Alphaçš„è¾“å‡ºå·®å¼‚: {diff_norm:.6f} (åº”è¯¥æ¥è¿‘0)")
            
            print("5ï¸âƒ£ æµ‹è¯•ä»…è§†è§‰æ¨¡å‹...")
            # æµ‹è¯•ç‹¬ç«‹çš„è§†è§‰æ¨¡å‹
            vision_outputs = model.vision_model(
                pixel_values=rgb_images,
                alpha_values=alpha_channels,
                return_dict=True
            )
            print(f"   âœ… è§†è§‰æ¨¡å‹æµ‹è¯•æˆåŠŸ: {vision_outputs.pooler_output.shape}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # æµ‹è¯•Alphaå‚æ•°
    print(f"\nğŸ” Alphaå‚æ•°æ£€æŸ¥:")
    alpha_params = get_alpha_parameters(model)
    for name, param in alpha_params:
        weight_norm = torch.norm(param).item()
        print(f"   {name}: {param.shape}, ||weight|| = {weight_norm:.6f}")
    
    print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    return True


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    test_alpha_blip_model()