import torch
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from transformers import BlipProcessor, BlipForConditionalGeneration
from transformers import get_linear_schedule_with_warmup
from PIL import Image
import json
from tqdm import tqdm
import os
from pathlib import Path
import cv2
import numpy as np
from torchvision import transforms
import random
import re
import sys
sys.path.append('/home/chenzc/cvd/model_blip')  # æ·»åŠ ä½ çš„é¡¹ç›®è·¯å¾„
from data.datasets_loader import VisualGenomeDataset, DualImageTransform  # ä¿®æ”¹ä¸ºå®é™…è·¯å¾„

def format_bbox_text(bbox, original_size, processed_size):
    """
    å°†bboxæ ¼å¼åŒ–ä¸ºå½’ä¸€åŒ–çš„xyxyæ–‡æœ¬æ ¼å¼
    Args:
        bbox: [x, y, width, height] - COCOæ ¼å¼
        original_size: (width, height) - åŸå§‹å›¾ç‰‡å°ºå¯¸
        processed_size: (width, height) - processorå¤„ç†åçš„å›¾ç‰‡å°ºå¯¸
    Returns:
        str: æ ¼å¼åŒ–çš„bboxæ–‡æœ¬ï¼Œå¦‚ "<xyxy>[0.123,0.456,0.789,0.012]</xyxy>"
    """
    x, y, w, h = bbox  # COCOæ ¼å¼: x,y,width,height
    orig_w, orig_h = original_size
    proc_w, proc_h = processed_size
    x1, y1, x2, y2 = x, y, x + w, y + h
    
    x_scale = proc_w / orig_w
    y_scale = proc_h / orig_h
    
    x1_scaled = x1 * x_scale
    y1_scaled = y1 * y_scale
    x2_scaled = x2 * x_scale
    y2_scaled = y2 * y_scale

    x1_norm = x1_scaled / proc_w
    y1_norm = y1_scaled / proc_h
    x2_norm = x2_scaled / proc_w
    y2_norm = y2_scaled / proc_h
    
    # ä¿ç•™3ä½å°æ•°
    return f"<xyxy>[{x1_norm:.3f},{y1_norm:.3f},{x2_norm:.3f},{y2_norm:.3f}]</xyxy>"

class VGBlipDataset(Dataset):
    """æ”¹è¿›çš„BLIPæ•°æ®é›†ï¼Œæ­£ç¡®å¤„ç†è¾“å…¥å’Œæ ‡ç­¾"""
    def __init__(self, 
                 region_descriptions_file,
                 image_data_file,
                 images_dir,
                 processor,
                 max_length=128
                 ):
        
        self.processor = processor
        self.max_length = max_length
        
        self.vg_dataset = VisualGenomeDataset(
            region_descriptions_file,
            image_data_file, 
            images_dir
        )
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®ä½œä¸ºè®­ç»ƒé›†
        total_size = len(self.vg_dataset)
        self.train_indices = list(range(total_size))
        self.val_indices = []  # ä¸ä½¿ç”¨éªŒè¯é›†
        
        print(f"Using full dataset for training: {len(self.train_indices)} samples")
    
    def __len__(self):
        return len(self.vg_dataset)
    
    def get_train_indices(self):
        return self.train_indices
    
    def get_val_indices(self):
        return self.val_indices
    
    def __getitem__(self, idx):
        # ä»VGæ•°æ®é›†è·å–æ ·æœ¬
        vg_sample = self.vg_dataset[idx]
        
        orig_image_pil = vg_sample['orig_image']
        target_text = vg_sample['text']
        bbox = vg_sample['bbox']
        
        # è·å–åŸå§‹å›¾ç‰‡å°ºå¯¸
        original_size = orig_image_pil.size
        
        # å…ˆç”¨processorå¤„ç†å›¾åƒè·å–å°ºå¯¸
        temp_processor_output = self.processor(
            orig_image_pil,
            return_tensors="pt",
            do_rescale=False
        )
        
        processed_pixel_values = temp_processor_output['pixel_values']
        processed_height = processed_pixel_values.shape[2]
        processed_width = processed_pixel_values.shape[3]
        processed_size = (processed_width, processed_height)
        
        # åˆ›å»ºbboxæ–‡æœ¬
        bbox_text = format_bbox_text(bbox, original_size, processed_size)
        
        # æ„å»ºå®Œæ•´çš„è¾“å…¥åºåˆ—ï¼šbbox + æè¿°
        full_sequence = f"{bbox_text} {target_text}"
        
        # å¤„ç†å›¾åƒå’Œå®Œæ•´åºåˆ—
        inputs = self.processor(
            orig_image_pil,
            full_sequence,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            do_rescale=False
        )
        
        # åˆ›å»ºlabelsï¼Œéœ€è¦maskæ‰bboxéƒ¨åˆ†
        # é¦–å…ˆè·å–bboxéƒ¨åˆ†çš„é•¿åº¦
        bbox_inputs = self.processor.tokenizer(
            bbox_text,
            return_tensors="pt",
            add_special_tokens=False  # ä¸æ·»åŠ ç‰¹æ®Štoken
        )
        bbox_length = bbox_inputs['input_ids'].shape[1]
        
        # åˆ›å»ºlabelsï¼Œbboxéƒ¨åˆ†è®¾ä¸º-100ï¼ˆå¿½ç•¥æŸå¤±ï¼‰
        labels = inputs['input_ids'].clone()
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªépadding tokençš„ä½ç½®
        first_token_pos = 1 if inputs['input_ids'][0, 0] == self.processor.tokenizer.bos_token_id else 0
        
        # maskæ‰bboxéƒ¨åˆ†ï¼ˆä»ç¬¬ä¸€ä¸ªtokenå¼€å§‹çš„bbox_lengthä¸ªtokenï¼‰
        labels[0, first_token_pos:first_token_pos + bbox_length] = -100
        
        # å¦‚æœæœ‰paddingï¼Œä¹Ÿè¦maskæ‰
        attention_mask = inputs['attention_mask']
        labels[attention_mask == 0] = -100
        
        return {
            'pixel_values': inputs['pixel_values'].squeeze(),
            'input_ids': inputs['input_ids'].squeeze(),
            'attention_mask': inputs['attention_mask'].squeeze(),
            'labels': labels.squeeze(),
            'bbox_text': bbox_text,  # ç”¨äºæµ‹è¯•æ—¶å‚è€ƒ
            'target_text': target_text  # ç”¨äºæµ‹è¯•æ—¶å‚è€ƒ
        }

class VGBlipTrainer:
    """æ”¹è¿›çš„VGæ•°æ®é›†BLIPè®­ç»ƒå™¨"""
    def __init__(self, 
                 model_name="Salesforce/blip-image-captioning-base",
                 vg_region_descriptions_file=None,
                 vg_image_data_file=None,
                 vg_images_dir=None,
                 output_dir="./blip_vg_finetuned",
                 max_length=128
                 ):
        
        self.model_name = model_name
        self.output_dir = output_dir
        self.processor = BlipProcessor.from_pretrained(model_name)
        self.model = BlipForConditionalGeneration.from_pretrained(model_name)
        
        # è·å–å›¾åƒå¤„ç†å™¨çš„é»˜è®¤å°ºå¯¸
        if hasattr(self.processor, 'image_processor'):
            img_proc = self.processor.image_processor
            if hasattr(img_proc, 'size'):
                self.default_image_size = img_proc.size
                print(f"BLIP processor default image size: {self.default_image_size}")
            else:
                self.default_image_size = {"height": 384, "width": 384}
                print(f"Using default BLIP image size: {self.default_image_size}")
        else:
            self.default_image_size = {"height": 384, "width": 384}
            print(f"Using fallback image size: {self.default_image_size}")

        # æ·»åŠ ç‰¹æ®Štoken
        special_tokens = ["<xyxy>", "</xyxy>"]
        self.processor.tokenizer.add_special_tokens({
            "additional_special_tokens": special_tokens
        })
        self.model.resize_token_embeddings(len(self.processor.tokenizer))
        print(f"Added xyxy bbox tokens, vocabulary size: {len(self.processor.tokenizer)}")
        
        # åˆ›å»ºæ•°æ®é›†
        if vg_region_descriptions_file and vg_image_data_file and vg_images_dir:
            print("Creating VG dataset...")
            self.dataset = VGBlipDataset(
                vg_region_descriptions_file,
                vg_image_data_file,
                vg_images_dir,
                self.processor,
                max_length=max_length
            )
            self.train_indices = self.dataset.get_train_indices()
            self.val_indices = self.dataset.get_val_indices()
        else:
            self.dataset = None
            self.train_indices = []
            self.val_indices = []
    
    def create_dataloader(self, batch_size=8, num_workers=4):
        """åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰"""
        if self.dataset is None:
            raise ValueError("Dataset not initialized")
        
        print(f"Using full dataset for training: {len(self.dataset)} samples")
        
        train_loader = DataLoader(
            self.dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True
        )
        
        return train_loader
    
    def run_test_on_samples(self, device, num_samples=8):
        """ä»è®­ç»ƒé›†æŠ½å–æ ·æœ¬è¿›è¡Œæµ‹è¯•"""
        self.model.eval()
        
        # ä»è®­ç»ƒé›†ä¸­éšæœºé€‰æ‹©æ ·æœ¬
        test_sample_indices = random.sample(self.train_indices, min(num_samples, len(self.train_indices)))
        test_results = []
        
        print(f"\n{'='*80}")
        print(f"TESTING ON {num_samples} RANDOM TRAINING SAMPLES")
        print(f"{'='*80}")
        
        with torch.no_grad():
            for i, idx in enumerate(test_sample_indices, 1):
                vg_sample = self.dataset.vg_dataset[idx]
                orig_image_pil = vg_sample['orig_image']
                ground_truth = vg_sample['text']
                bbox = vg_sample['bbox']
                
                original_size = orig_image_pil.size
                processed_size = (self.default_image_size["width"], self.default_image_size["height"])
                
                # æ„å»ºè¾“å…¥promptï¼ˆåªæœ‰bboxï¼‰
                bbox_text = format_bbox_text(bbox, original_size, processed_size)
                
                # ä½¿ç”¨bboxä½œä¸ºpromptç”Ÿæˆ
                inputs = self.processor(
                    orig_image_pil, 
                    bbox_text,
                    return_tensors="pt",
                    do_rescale=False
                ).to(device)
                
                # ç”Ÿæˆæè¿°
                generated_ids = self.model.generate(
                    pixel_values=inputs['pixel_values'],
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_new_tokens=30,
                    num_beams=3,
                    do_sample=False,
                    early_stopping=True
                )
                
                # è§£ç æ—¶è·³è¿‡è¾“å…¥éƒ¨åˆ†
                input_length = inputs['input_ids'].shape[1]
                generated_text = self.processor.decode(
                    generated_ids[0][input_length:], 
                    skip_special_tokens=True
                ).strip()
                
                # å¦‚æœç”Ÿæˆä¸ºç©ºï¼Œå°è¯•å®Œæ•´è§£ç å¹¶æ¸…ç†
                if not generated_text:
                    full_text = self.processor.decode(generated_ids[0], skip_special_tokens=True)
                    generated_text = re.sub(r'<xyxy>\[.*?\]</xyxy>', '', full_text).strip()
                
                # ç«‹å³æ‰“å°ç»“æœ
                print(f"\n--- Sample {i} (Dataset Index: {idx}) ---")
                print(f"BBox (COCO format): {bbox}")
                print(f"BBox Text: {bbox_text}")
                print(f"Ground Truth: {ground_truth}")
                print(f"Generated:    {generated_text}")
                print("-" * 60)
                
                test_results.append({
                    'sample_idx': idx,
                    'bbox': bbox,
                    'bbox_text': bbox_text,
                    'ground_truth': ground_truth,
                    'generated_text': generated_text
                })
        
        print(f"{'='*80}\n")
        self.model.train()
        return test_results
    
    def print_test_results(self, test_results, step):
        """æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦"""
        print(f"\n{'='*80}")
        print(f"TEST RESULTS SUMMARY AT STEP {step}")
        print(f"{'='*80}")
        print(f"Total samples tested: {len(test_results)}")
        print(f"Average generated length: {sum(len(r['generated_text'].split()) for r in test_results) / len(test_results):.1f} words")
        print(f"{'='*80}\n")
    
    def save_test_results(self, test_results, step):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        results_dir = Path(self.output_dir) / "test_results"
        results_dir.mkdir(parents=True, exist_ok=True)
        
        results_file = results_dir / f"test_results_step_{step}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'step': step,
                'results': test_results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"Test results saved to: {results_file}")
    
    def train(self,
              num_epochs=3,
              batch_size=8,
              learning_rate=1e-5,
              warmup_steps=200,
              test_steps=100,
              max_grad_norm=1.0):
        """è®­ç»ƒæ¨¡å‹ï¼ˆæ— æ—©åœç‰ˆæœ¬ï¼‰"""
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = self.create_dataloader(batch_size=batch_size)
        
        # è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)
        total_steps = len(train_loader) * num_epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )
        
        # è®¾ç½®è®¾å¤‡
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(device)
        
        print(f"Training on device: {device}")
        print(f"Total training steps: {total_steps}")
        print(f"Training samples: {len(self.train_indices)}")
        print(f"Test every {test_steps} steps")
        
        # è®­ç»ƒå¾ªç¯
        global_step = 0
        
        for epoch in range(num_epochs):
            print(f"\n=== Epoch {epoch + 1}/{num_epochs} ===")
            
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            total_train_loss = 0
            train_bar = tqdm(train_loader, desc=f"Training Epoch {epoch+1}")
            
            for step, batch in enumerate(train_bar):
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                        for k, v in batch.items()}
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    pixel_values=batch['pixel_values'],
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    labels=batch['labels']
                )
                
                loss = outputs.loss
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)
                
                optimizer.step()
                scheduler.step()
                
                total_train_loss += loss.item()
                global_step += 1
                
                train_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'lr': f'{scheduler.get_last_lr()[0]:.2e}',
                    'step': global_step
                })
                
                # å®šæœŸæµ‹è¯•
                if global_step % test_steps == 0:
                    print(f"\nRunning test at step {global_step}...")
                    test_results = self.run_test_on_samples(device, num_samples=8)
                    self.print_test_results(test_results, global_step)
                    self.save_test_results(test_results, global_step)
            
            avg_train_loss = total_train_loss / len(train_loader)
            
            print(f"\nEpoch {epoch+1} Summary:")
            print(f"  Average Train Loss: {avg_train_loss:.4f}")
            print(f"  Total Steps: {global_step}")
            
            # ä¿å­˜æ¯ä¸ªepochçš„æ¨¡å‹
            self.save_model(f"model_epoch_{epoch+1}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_model("final_model")
        
        avg_final_loss = total_train_loss / len(train_loader)
        print(f"\nTraining completed! Final average loss: {avg_final_loss:.4f}")
        return avg_final_loss
    
    def save_model(self, checkpoint_name):
        """ä¿å­˜æ¨¡å‹"""
        save_path = Path(self.output_dir) / checkpoint_name
        save_path.mkdir(parents=True, exist_ok=True)
        
        self.model.save_pretrained(save_path)
        self.processor.save_pretrained(save_path)
        
        print(f"Model saved to {save_path}")
    
    def load_model(self, checkpoint_path):
        """åŠ è½½æ¨¡å‹"""
        self.model = BlipForConditionalGeneration.from_pretrained(checkpoint_path)
        self.processor = BlipProcessor.from_pretrained(checkpoint_path)
        print(f"Model loaded from {checkpoint_path}")
    
    def test_model(self, image_path, bbox=None, prompt="Describe this region:"):
        """æµ‹è¯•æ¨¡å‹ç”Ÿæˆèƒ½åŠ›"""
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(device)
        self.model.eval()
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        original_size = image.size
        
        # æ·»åŠ bboxåˆ°prompt
        if bbox:
            processed_size = (self.default_image_size["width"], self.default_image_size["height"])
            bbox_text = format_bbox_text(bbox, original_size, processed_size)
            full_prompt = f"{bbox_text} {prompt}"
            
            print(f"Original image size: {original_size}")
            print(f"Processed image size: {processed_size}")
            print(f"Original bbox (COCO): {bbox}")
            print(f"Formatted bbox text: {bbox_text}")
        else:
            full_prompt = prompt
        
        # å¤„ç†è¾“å…¥
        inputs = self.processor(image, full_prompt, return_tensors="pt", do_rescale=False).to(device)
        
        # ç”Ÿæˆæ–‡æœ¬
        with torch.no_grad():
            generated_ids = self.model.generate(
                pixel_values=inputs['pixel_values'],
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_new_tokens=30,
                num_beams=4,
                do_sample=True,
            )
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        if bbox:
            # è·³è¿‡è¾“å…¥éƒ¨åˆ†
            input_length = inputs['input_ids'].shape[1]
            generated_text = self.processor.decode(
                generated_ids[0][input_length:], 
                skip_special_tokens=True
            ).strip()
        else:
            generated_text = self.processor.decode(generated_ids[0], skip_special_tokens=True)
        
        return generated_text

    def test_random_samples(self, device, num_samples=8):
        """ç‹¬ç«‹çš„æµ‹è¯•å‡½æ•°ï¼Œåªæµ‹è¯•ä¸ä¿å­˜"""
        print(f"\nğŸ” Testing {num_samples} random samples from training set...")
        test_results = self.run_test_on_samples(device, num_samples)
        return test_results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    random.seed(42)
    np.random.seed(42)
    
    # é…ç½®è·¯å¾„
    config = {
        'vg_region_descriptions_file': "/home/chenzc/cvd/model_blip/data/data/visual_genome/VG_train_105077/region_descriptions.json",
        'vg_image_data_file': "/home/chenzc/cvd/model_blip/data/data/visual_genome/VG_train_105077/image_data.json", 
        'vg_images_dir': "/home/chenzc/cvd/model_blip/data/data/visual_genome/VG_100K_all",
        'output_dir': "./blip_vg_finetuned_v2",
        'model_name': "Salesforce/blip-image-captioning-base"
    }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['output_dir'], exist_ok=True)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = VGBlipTrainer(
        model_name=config['model_name'],
        vg_region_descriptions_file=config['vg_region_descriptions_file'],
        vg_image_data_file=config['vg_image_data_file'],
        vg_images_dir=config['vg_images_dir'],
        output_dir=config['output_dir'],
        max_length=128
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("Starting training...")
    final_loss = trainer.train(
        num_epochs=1,  # åªè®­ç»ƒä¸€ä¸ªepoch
        batch_size=16,  # æ ¹æ®GPUå†…å­˜è°ƒæ•´
        learning_rate=2e-5,
        test_steps=200  # æ¯200æ­¥æµ‹è¯•ä¸€æ¬¡
    )
    
    print(f"Training completed with final loss: {final_loss:.4f}")
    
    # æœ€ç»ˆæµ‹è¯•
    print("Running final test...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    final_test_results = trainer.test_random_samples(device, num_samples=10)